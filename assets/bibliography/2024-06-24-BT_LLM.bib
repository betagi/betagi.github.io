% Encoding: UTF-8
@book{Martin2024,
    author    = {Jurafsky, Daniel and Martin, James H.},
    title     = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
    edition   = {3rd},
    year      = {2024},
    publisher = {Pearson},
    address   = {Boston, MA},
    url       = {https://web.stanford.edu/~jurafsky/slp3/ed3bookfeb3_2024.pdf}
}

@misc{Chen23,
Author = {Daoyuan Chen and Yilun Huang and Zhijian Ma and Hesen Chen and Xuchen Pan and Ce Ge and Dawei Gao and Yuexiang Xie and Zhaoyang Liu and Jinyang Gao and Yaliang Li and Bolin Ding and Jingren Zhou},
Title = {Data-Juicer: A One-Stop Data Processing System for Large Language Models},
Year = {2023},
Eprint = {arXiv:2309.02033},
}

@misc{Ba2016, 
Author = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
Title = {Layer Normalization},
Year = {2016},
Eprint = {arXiv:1607.06450},
}

@misc{Shan23,
Author = {Shawn Shan and Wenxin Ding and Josephine Passananti and Stanley Wu and Haitao Zheng and Ben Y. Zhao},
Title = {Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models},
Year = {2023},
Eprint = {arXiv:2310.13828},
}

@misc{Geng24,
Author = {Wei Zou and Runpeng Geng and Binghui Wang and Jinyuan Jia},
Title = {PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models},
Year = {2024},
Eprint = {arXiv:2402.07867},
}

@misc{bge_embedding,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{Faiss,
      title={The Faiss library},
      author={Matthijs Douze and Alexandr Guzhva and Chengqi Deng and Jeff Johnson and Gergely Szilvasy and Pierre-Emmanuel Mazaré and Maria Lomeli and Lucas Hosseini and Hervé Jégou},
      year={2024},
      eprint={2401.08281},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{LLaMa,
Author = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
Title = {LLaMA: Open and Efficient Foundation Language Models},
Year = {2023},
Eprint = {arXiv:2302.13971},
}

@misc{T5,
Author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
Title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
Year = {2019},
Eprint = {arXiv:1910.10683},
}

@misc{OpenAI23,
Author = { OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and  Michael and  Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
Title = {GPT-4 Technical Report},
Year = {2023},
Eprint = {arXiv:2303.08774},
}

@misc{Brown20, 
Author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
Title = {Language Models are Few-Shot Learners},
Year = {2020},
Eprint = {arXiv:2005.14165},
}

@misc{Fang24,
Author = {Xi Fang and Weijie Xu and Fiona Anting Tan and Jiani Zhang and Ziqing Hu and Yanjun Qi and Scott Nickleach and Diego Socolinsky and Srinivasan Sengamedu and Christos Faloutsos},
Title = {Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding -- A Survey},
Year = {2024},
Eprint = {arXiv:2402.17944},
}

@misc{Guo24,
Author = {Luyang Lin and Lingzhi Wang and Jinsong Guo and Kam-Fai Wong},
Title = {Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception},
Year = {2024},
Eprint = {arXiv:2403.14896},
}

@misc{Wei22,
Author = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and Ed H. Chi and Tatsunori Hashimoto and Oriol Vinyals and Percy Liang and Jeff Dean and William Fedus},
Title = {Emergent Abilities of Large Language Models},
Year = {2022},
Eprint = {arXiv:2206.07682},
}

@misc{Isabel23,
Author = {Isabel O. Gallegos and Ryan A. Rossi and Joe Barrow and Md Mehrab Tanjim and Sungchul Kim and Franck Dernoncourt and Tong Yu and Ruiyi Zhang and Nesreen K. Ahmed},
Title = {Bias and Fairness in Large Language Models: A Survey},
Year = {2023},
Eprint = {arXiv:2309.00770},
}

@misc{Tang23, 
Author = {Xiaotian Zhou and Qian Wang and Xiaofeng Wang and Haixu Tang and Xiaozhong Liu},
Title = {Large Language Model Soft Ideologization via AI-Self-Consciousness},
Year = {2023},
Eprint = {arXiv:2309.16167},
}

@misc{COT22,
Author = {Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc Le and Ed Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
Title = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
Year = {2022},
Eprint = {arXiv:2203.11171},
}

@misc{Lin24,
Author = {Demiao Lin},
Title = {Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition},
Year = {2024},
Eprint = {arXiv:2401.12599},
}

@misc{Zhang23, 
Author = {Shengyu Zhang and Linfeng Dong and Xiaoya Li and Sen Zhang and Xiaofei Sun and Shuhe Wang and Jiwei Li and Runyi Hu and Tianwei Zhang and Fei Wu and Guoyin Wang},
Title = {Instruction Tuning for Large Language Models: A Survey},
Year = {2023},
Eprint = {arXiv:2308.10792},
}

@misc{RAG20,
Author = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
Title = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
Year = {2020},
Eprint = {arXiv:2005.11401},
}

@misc{Fu24,
Author = {Xue-Yong Fu and Md Tahmid Rahman Laskar and Elena Khasanova and Cheng Chen and Shashi Bhushan TN},
Title = {Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?},
Year = {2024},
Eprint = {arXiv:2402.00841},
}

@misc{PalM-E,
Author = {Danny Driess and Fei Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Pete Florence},
Title = {PaLM-E: An Embodied Multimodal Language Model},
Year = {2023},
Eprint = {arXiv:2303.03378},
}

@misc{Hartmann23,
Author = {Jochen Hartmann and Jasper Schwenzow and Maximilian Witte},
Title = {The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation},
Year = {2023},
Eprint = {arXiv:2301.01768},
}

@article{DeepB,
title = {Deep Blue},
journal = {Artificial Intelligence},
volume = {134},
number = {1},
pages = {57-83},
year = {2002},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(01)00129-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370201001291},
author = {Murray Campbell and A.Joseph Hoane and Feng-hsiung Hsu},
keywords = {Computer chess, Game tree search, Parallel search, Selective search, Search extensions, Evaluation function},
abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: •a single-chip chess search engine,•a massively parallel system with multiple levels of parallelism,•a strong emphasis on search extensions,•a complex evaluation function, and•effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue.}
}

@misc{Zeng24,
Author = {Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},
Title = {TinyLlama: An Open-Source Small Language Model},
Year = {2024},
Eprint = {arXiv:2401.02385},
}

@article{Bricken23,
       title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
       author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
       year={2023},
       journal={Transformer Circuits Thread},
       note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
    }

@inproceedings{GLM22,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}
    
@misc{Mixtra24,
Author = {Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
Title = {Mixtral of Experts},
Year = {2024},
Eprint = {arXiv:2401.04088},
}

@misc{Liu23,
Author = {Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and Lili Yu and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
Title = {LIMA: Less Is More for Alignment},
Year = {2023},
Eprint = {arXiv:2305.11206},
}
    
@misc{Wang20,
Author = {Yu-An Wang and Yun-Nung Chen},
Title = {What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding},
Year = {2020},
Eprint = {arXiv:2010.04903},
}

@misc{Hoffmann22,
Author = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
Title = {Training Compute-Optimal Large Language Models},
Year = {2022},
Eprint = {arXiv:2203.15556},
}

@misc{Liao24,
Author = {Yidong Liao and Chris Ferrie},
Title = {GPT on a Quantum Computer},
Year = {2024},
Eprint = {arXiv:2403.09418},
}

@misc{Devlin19,
Author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
Title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
Year = {2018},
Eprint = {arXiv:1810.04805},
}

@article{Olah20,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}

@misc{Jiang24,
Author = {Yibo Jiang and Goutham Rajendran and Pradeep Ravikumar and Bryon Aragam and Victor Veitch},
Title = {On the Origins of Linear Representations in Large Language Models},
Year = {2024},
Eprint = {arXiv:2403.03867},
}

@misc{Marks23,
Author = {Samuel Marks and Max Tegmark},
Title = {The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets},
Year = {2023},
Eprint = {arXiv:2310.06824},
}

@misc{Radford17,
Author = {Alec Radford and Rafal Jozefowicz and Ilya Sutskever},
Title = {Learning to Generate Reviews and Discovering Sentiment},
Year = {2017},
Eprint = {arXiv:1704.01444},
}

@misc{Vuckovic20,
Author = {James Vuckovic and Aristide Baratin and Remi Tachet des Combes},
Title = {A Mathematical Theory of Attention},
Year = {2020},
Eprint = {arXiv:2007.02876},
}

@misc{Sander22,
Author = {Michael E. Sander and Pierre Ablin and Mathieu Blondel and Gabriel Peyré},
Title = {Sinkformers: Transformers with Doubly Stochastic Attention},
Year = {2021},
Eprint = {arXiv:2110.11773},
}

@misc{Geshkovski23,
Author = {Borjan Geshkovski and Cyril Letrouit and Yury Polyanskiy and Philippe Rigollet},
Title = {A mathematical perspective on Transformers},
Year = {2023},
Eprint = {arXiv:2312.10794},
}

@misc{ChoiY2019,
Author = {Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
Title = {The Curious Case of Neural Text Degeneration},
Year = {2019},
Eprint = {arXiv:1904.09751},
}

@misc{Nwankpa2018,
Author = {Chigozie Nwankpa and Winifred Ijomah and Anthony Gachagan and Stephen Marshall},
Title = {Activation Functions: Comparison of trends in Practice and Research for Deep Learning},
Year = {2018},
Eprint = {arXiv:1811.03378},
}

@misc{Dar2022,
Author = {Guy Dar and Mor Geva and Ankit Gupta and Jonathan Berant},
Title = {Analyzing Transformers in Embedding Space},
Year = {2022},
Eprint = {arXiv:2209.02535},
}

@article{Simon23,
Author = {Simon Frieder and Julius Berner and Philipp Petersen and Thomas Lukasiewicz},
Title = {Large Language Models for Mathematicians},
Year = {2023},
Eprint = {arXiv:2312.04556},
Howpublished = {International Mathematical News 254 (2023) 1-20},
}

@misc{Dong22,
Author = {Damai Dai and Li Dong and Yaru Hao and Zhifang Sui and Baobao Chang and Furu Wei},
Title = {Knowledge Neurons in Pretrained Transformers},
Year = {2021},
Eprint = {arXiv:2104.08696},
}

@misc{LeeA23,
Author = {Neel Nanda and Andrew Lee and Martin Wattenberg},
Title = {Emergent Linear Representations in World Models of Self-Supervised Sequence Models},
Year = {2023},
Eprint = {arXiv:2309.00941},
}

@misc{Conmy23,
Author = {Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
Title = {Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small},
Year = {2022},
Eprint = {arXiv:2211.00593},
}
@Article{Bengio00,
  author    = {Yoshua Bengio and Réjean Ducharme and Pascal Vincent},
  title     = {A Neural Probabilistic Language Model},
  journal   = {Journal of Machine Learning Research},
  volume    = {3},
  year      = {2003},
  pages     = {1137--1155},
  url       = {https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf}
}

@misc{Naveed24,
Author = {Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian},
Title = {A Comprehensive Overview of Large Language Models},
Year = {2023},
Eprint = {arXiv:2307.06435},
}

@misc{GRU14,
Author = {Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio},
Title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
Year = {2014},
Eprint = {arXiv:1412.3555},
}

@article{LSTM97,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = {nov},
pages = {1735–1780},
numpages = {46}
}

@misc{Vaswani17,
Author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
Title = {Attention Is All You Need},
Year = {2017},
Eprint = {arXiv:1706.03762},
}

@inproceedings{Radford18,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:49313245}
}

@misc{Kudo18,
Author = {Taku Kudo},
Title = {Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates},
Year = {2018},
Eprint = {arXiv:1804.10959},
}

@misc{Birch16,
Author = {Rico Sennrich and Barry Haddow and Alexandra Birch},
Title = {Neural Machine Translation of Rare Words with Subword Units},
Year = {2015},
Eprint = {arXiv:1508.07909},
}

@misc{Minaee24,
Author = {Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
Title = {Large Language Models: A Survey},
Year = {2024},
Eprint = {arXiv:2402.06196},
}

@inproceedings{Ghojogh20,
  title={Attention Mechanism, Transformers, BERT, and GPT: Tutorial and Survey},
  author={Benyamin Ghojogh and Ali Ghodsi},
  year={2020},
  url={https://api.semanticscholar.org/CorpusID:242275301}
}

@misc{Su2021,
Author = {Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
Title = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
Year = {2021},
Eprint = {arXiv:2104.09864},
}

@misc{Kaplan20,
Author = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
Title = {Scaling Laws for Neural Language Models},
Year = {2020},
Eprint = {arXiv:2001.08361},
}

@misc{Doglas2023,
Author = {Michael R. Douglas},
Title = {Large Language Models},
Year = {2023},
Eprint = {arXiv:2307.05782},
}

@misc{Mikolov2013,
Author = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
Title = {Efficient Estimation of Word Representations in Vector Space},
Year = {2013},
Eprint = {arXiv:1301.3781},
}
@inproceedings{Pennington2014,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
} 

@misc{Sennrich16,
Author = {Rico Sennrich and Barry Haddow and Alexandra Birch},
Title = {Neural Machine Translation of Rare Words with Subword Units},
Year = {2015},
Eprint = {arXiv:1508.07909},
}

@misc{CohnT15,
Author = {Ekaterina Vylomova and Laura Rimell and Trevor Cohn and Timothy Baldwin},
Title = {Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning},
Year = {2015},
Eprint = {arXiv:1509.01692},
}

@misc{Kocmi2017,
Author = {Tom Kocmi and Ondřej Bojar},
Title = {An Exploration of Word Embedding Initialization in Deep-Learning Tasks},
Year = {2017},
Eprint = {arXiv:1711.09160},
}

@inproceedings{Luong2015,
    title = "Bilingual Word Representations with Monolingual Quality in Mind",
    author = "Luong, Thang  and
      Pham, Hieu  and
      Manning, Christopher D.",
    editor = "Blunsom, Phil  and
      Cohen, Shay  and
      Dhillon, Paramveer  and
      Liang, Percy",
    booktitle = "Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing",
    month = jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-1521",
    doi = "10.3115/v1/W15-1521",
    pages = "151--159",
}
@article{Elhage21,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{Wang2019,
Author = {Bin Wang and Angela Wang and Fenxiao Chen and Yuncheng Wang and C. -C. Jay Kuo},
Title = {Evaluating Word Embedding Models: Methods and Experimental Results},
Year = {2019},
Eprint = {arXiv:1901.09785},
Howpublished = {APSIPA Transactions on Signal and Information Processing 8 (2019)
  e19},
Doi = {10.1017/ATSIP.2019.12},
}

@misc{Haviv22,
Author = {Adi Haviv and Ori Ram and Ofir Press and Peter Izsak and Omer Levy},
Title = {Transformer Language Models without Positional Encodings Still Learn Positional Information},
Year = {2022},
Eprint = {arXiv:2203.16634},
}

@misc{Reddy2023,
Author = {Amirhossein Kazemnejad and Inkit Padhi and Karthikeyan Natesan Ramamurthy and Payel Das and Siva Reddy},
Title = {The Impact of Positional Encoding on Length Generalization in Transformers},
Year = {2023},
Eprint = {arXiv:2305.19466},
}

@misc{He2015,
Author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
Title = {Deep Residual Learning for Image Recognition},
Year = {2015},
Eprint = {arXiv:1512.03385},
}

@misc{Brody2023,
Author = {Shaked Brody and Uri Alon and Eran Yahav},
Title = {On the Expressivity Role of LayerNorm in Transformers' Attention},
Year = {2023},
Eprint = {arXiv:2305.02582},
}

@misc{Molina23,
Author = {Raul Molina},
Title = {Traveling Words: A Geometric Interpretation of Transformers},
Year = {2023},
Eprint = {arXiv:2309.07315},
}

